# -*- coding:utf-8 -*-

from numpy import mat,ones

'''
Logestic回归可以看成是一个无隐层的神经网络。由于其结构简单，一般不需要再特地去画出神经网络拓扑，直接算就行了。
Logestic回归的关键词是回归系数（即权值）的学习。对于分类这种任务而言，由于回归计算得到的值是连续的而非离散的，所以一般还需要一个
处理函数将连续的值转化为离散的分类值。常见的函数就是sigmoid，比如将计算结果大于0.5的分类成1，否则分类成0。
'''
import math
def sigmoid(x):
    return 1.0/(1+math.exp(-x))

'''
梯度上升:
对于一个在定义域上都可导的函数f(x)而言，要找到它的极大值，一个可行的方法是任选一个x0，然后找x1 = x0 + △x
其中△x是α*f'(x0)，α称步长（或学习率）。这个△x的意义就是，如果当前点x0处于上升阶段，并且斜率较大，那么x的迭代会比较快地增长
当x接近极大值，即f'(x)等于0的情况，f'(x)会变得很小，因此迭代增长速率也会逐渐变慢。
*反过来的，如果说x = x - △x的话，那么x的迭代会逐渐逼近f(x)的极小值，方法也称梯度下降了。
'''

'''
下面的训练算法采用“梯度上升”法来迭代更新回归系数。
这里很重要的一个问题是f(x)到底是什么… 事实告诉我们，f(x)在这里是对回归系数值的似然估计函数，因为我对什么是最大似然估计还处于懵逼状态，所以也不好妄言…
总之记住两点，1. 似然估计函数达到最大值，即最大似然估计时，回归系数最贴合实际结果。因此修正回归系数的目的是为了让这个函数的取值能够最大
2. 似然估计函数对回归系数的某个分量wi求偏导，可以化为这个式子：(y - y') * xi，也就是说，上面梯度上升方法中的△x具体到这里可以写成
α * (y - y') * xi，其中y'是当前样本的估计值而y是实际值。xi是这个样本和wi对应的那个输入向量的分量值
知道了△x之后，根据梯度上升的方法去找到似然估计函数的最大值，最终便可确定回归系数的取值，模型也就训练完成了，回归也就完成了。
'''

'''
如果还记得NN章里提到的梯度下降的应用，当时的目的是通过梯度下降去找到损失函数的最小值。而损失函数当时是定义为(y-y')^2，和这里的似然估计函数有异曲同工之妙
然而为什么NN里是梯度下降求最小，这里又是梯度上升求最大… 就不是很清楚了
'''

def gradAcsent(dataSet,labels):
    '''
    采取梯度上升策略迭代更新回归系数，训练完成以训练次数为标准
    :param dataSet:    numpy.mat类数据
    :param labels:    numpy.mat类数据
    :return:
    '''
    m,n = dataSet.shape
    weights = ones((n,1))    # 以1初始化各个回归系数，生成的虽然是array，但是可以直接拿去和合适的矩阵运算
    itertime = 500    # 注意，itertime不是样本迭代次数，而是样本迭代轮数！
    alpha = 0.1    # 步长（叫学习率也行）
    for i in range(itertime):
        predict = sigmoid(dataSet * weights)    # 注意此时dataSet和weights都是矩阵，所以sigmoid处理的是一个m行1列的矩阵，输出的predict自然也是m*1矩阵
        # predict是当前回归系数情况下各个样本的预测结果
        shift = labels - predict  # 计算实际结果和预测结果的偏差量
        weights = weights + alpha * dataSet.transpose() * shift    # 更新回归系数，注意这里是批处理，但是并不用除以m。
        # 根据梯度上升迭代公式的定义，每个样本都应当更新一次回归系数，因此本身确实就是累加的，矩阵乘法的时候直接累加没毛病

    return weights

'''
上面的是基于梯度上升进行回归系数学习的算法。这个算法很明显，是“批处理”形式的，类比NN中的ABP算法
由于处理的最小单位落在数据集层面，所以当数据集较大时，效率可能较差。类别NN中ABP可以转BP，这里的算法也可以转化成单个样本就更新一次回归系数的做法。
那样的做法中，学习回归系数的方式被称为“随机梯度上升”，由于样本的排布不一定有规律，所以相当于为回归系数的迭代更新带来了一些随机的不确定因素
随机梯度上升算法显然计算更多更频繁，但是也能更快找到极大值。同时在NN中也提到过，随机梯度上升能够更好地避免“陷入”极小/大值陷阱。
'''

def stocGradAscent(dataSet,labels):
    '''
    随机梯度上升，还是以训练次数为标记
    :param dataSet: 这次数据集可以是array类型了
    :param labels: 同上
    :return:
    '''
    m,n = dataSet.shape
    weights = ones(n)    # 注意，生成的是一个array
    alpha = 0.1
    for i in range(m):
        sample = dataSet[i]
        label = labels[i]
        predict = sigmoid(sum(sample * weights))    # 单个样本的估计值，要sum一下是因为此时sample和weights都是array，只是相乘还是一个array，没有相加的效果
        weights = weights + alpha * sample *  (label - predict)  # 每个样本分析完之后都更新一次回归系数
    return weights

'''
如果有实际的训练数据测试一下上面这个函数，你会发现效果远远不如上面的那个好。
直接比较这两个是不公平的，因为上面的函数训练的是几百轮，而下面的只训练一轮几百次而已。
可以人为地在下面的函数的循环外面套一层轮次控制，也来个几百轮，这样就会好很多了。代码就不写了
'''
'''
接下来还要讨论一个和 logestic不那么相关，但是ML中普遍可能遇到的一个问题，就是缺失值的问题。
在西瓜书的决策树一章中曾经提到过缺失值如何处理，当时提到可以使用“无视”以及“量子化分布”两种可能的策略
事实上，汇总一些缺失值处理的可能方法：
1. 直接忽略本样本
2. 用其他不缺此属性值的样本的属性值均值代替本样本的值
3. 用相似样本（如何判相似也是个问题）的值代替本样本的值
4. 用特殊值填补缺失值
5. 基于其他有值的样本，和其他机器学习手段为本样本预测一个值

在logestic回归中，可采用方案4，用0来填补缺失值。这么做是因为当xi=0时带入sigmoid的结果是0.5，这个结果不偏不倚，不会对预测结果有影响
另外原则上样本采样过程中也尽量不会将属性赋值为0，所以0也算是一个特殊值
'''
# -*- coding:utf-8 -*-

'''

'''

from .DecisionTree import *
from numpy import array
from Queue import Queue

'''
决策树算法有一种去除过拟合的方法称为剪枝。顾名思义，当模型对一些训练集的数据过于敏感，出现过拟合的时候现象就是
树上会多出很多不必要的分支。剪枝大概可分成预剪枝和后剪枝。
预剪枝在建立模型树的时候进行操作，采用贪心的思想，将一些可能出现过拟合的枝条直接做不生成处理。
后剪枝在模型树建立之后，由下至上地进行检查，去除一些出现过拟合的枝条。

那么怎么样才能判断一个枝条是否过拟合。此时除了训练集之外我们还需要构造一个验证集作为检查的工具
简单来说，在一个决策树的节点上，我们决定了一个分类属性并且取了其具体一个值之后，得到了分了出来的子集。按照之前说过的决策树构造算法
这个子集很可能 还要做（预剪枝）/做了（后剪枝） 进一步的分类，如果我们就此打住，把当前子集的众数label作为分类结果，就相当于做了一个剪枝操作
然后将上面说的验证集中的数据导进来，看剪枝和不剪枝情况下验证集准确率哪个高。若是不剪枝高那就维持枝条，反则确定要剪枝。
'''

'''
非离散值的决策树
对于属性可能的取值不是离散值集合的情况，可以先按照从小到大的顺序，将这个属性在当前集合中出现过的所有取值都列出来，
然后依次计算两个点之间中点值，基于这些中点值对当前数据集进行分类（但是不一定是<或>中点值；为了保证最终得到的分类树和训练集的相关性，
对于(Ai + A(i+1) ) / 2这个中点值，会采用分类标准是a <= Ai 和 a > Ai来分类）
每一种分类，都会将数据集二分，计算所有这些二分带来的信息增益，就可以得到一个信息增益最大的中点值，继而找到那个Ai
最终，取值是非离散值的属性作为分类属性，体现在决策树上的节点，是一个度数是2的二叉节点，而此节点的判断条件就是a <= Ai了
'''

'''
缺失值的处理
实际学习过程中，训练集中的样本很可能不是完整的，比如存在一些记录，其中的一些属性的值是None。那么面临的问题就是，
1. 这条记录的空值属性an，以an作为分类属性的信息增益怎么算
2. 确定某个分类属性之后，划分子集时此分类属性值为空的记录dn改归属到什么子集去
MD这个处理还蛮复杂的… 一时半会儿说不清原理，还是看西瓜书吧，公式都在书上。总体的要义是：
针对问题1，采用“无视”的策略，基于an属性有值的那些记录，和一个描述有值/空值情况的参数，整合算出一个信息增益的参考值来
针对问题2，根据训练集中其他有值记录的分类分布情况，将这个记录“量子化”分散到各个不同的子集，并记录分布的概率作为权值。\
基于这样的“量子化分布”来进一步计算各子集的最优分类属性，从而构建一棵树

需要注意的是，问题2的解决策略和问题1的解决策略是一体的。比如最开始的一次分类，要将各记录的权值都初始化为1，方便统一计算
而一个子集中若是出现了类似于0.75个记录这种情况，在进一步计算这个子集的信息增益过程中，这个系数也始终要跟着这个记录走，不能漏掉
'''

# 妈的搞不出来，为了不浪费太多时间，先跳过吧
# 如何对决策树进行剪枝 以及 带有缺失值的训练集训练
# def cutBranch(beforeTree,afterTree,validateInfo):
#     validateSet,validateLabel = validateInfo
#     beforeCount,afterCount = 0,0
#     for validateRow in validateSet:
#         if classify(validateRow,beforeTree):
#             beforeCount += 1
#         if classify(validateRow,afterTree):
#             afterCount += 1
#     return beforeCount >= afterCount
#
# def createTree2(dataSet,labels,usedAttrs,validateInfo):
#     attrIdx = getClassifyAttr2(dataSet,labels,usedAttrs)
#     resTree = {attrIdx: {}}



